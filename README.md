![Alpaca-CoT](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/Alpaca-CoT-2.jpg)
# Evolving Alpaca: An Empirical Study on Instruction Tuning for Large Language Models (Alpaca-CoT)

This is the repository for the `Evolving Alpaca` project, which aims to extensively collect instruction-tuning datasets and conduct an in-depth empirical study based on LLaMA model [1].  `Evolving` is used to describe the continuous expansion of our instruction-tuning data collection, which will continuously enhance [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)'s instruction-following capabilities.

## Overview

[1]: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

## Data Collection 
### Statistics
![data collection statistics](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/piechart.png)



## Quantitative Analysis
  3.1 using different instruction data
  3.2 using different sizes of models
  3.3 Comparison with Alpaca and LLaMA


## Instruction Finetuning Based on LLaMA
### Setup

### Training

### Inference

## Citation
Please cite the repo if you use the data collection, code, and experimental findings in this repo. 
